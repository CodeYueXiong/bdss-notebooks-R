---
title: "07_3_Machine_Learning_Models"
output: pdf_document
author: Yue Xiong, LMU, yue.xiong@stat.uni-muenchen.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning -- Model Training and Evaluation
-----

# Introduction

In this tutorial, we'll discuss how to formulate a policy problem or a social science question in the machine learning framework; how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `mlr3` package in R. 

This tutorial is based on chapter "Machine Learning" of [Big Data and Social Science](https://coleridge-initiative.github.io/big-data-and-social-science/).

## Setup

```{r, message=FALSE, warning=FALSE}
library('dbplyr')  # load the corresponding libraries
library('dplyr')
library('RSQLite')
library('glue')
library('caret')
```

```{r}
# Establish a connection to the ncdoc.db database
database_path = "F:/hiwi_work_notebook/ncdoc.db"
conn = DBI::dbConnect(SQLite(), database_path)
src_dbi(conn)
```

# Problem Formulation
---
  
Our Machine Learning Problem
Of all prisoners released, we would like to predict who is likely to reenter jail within *5* years of the day we make our prediction. For instance, say it is Jan 1, 2009 and we want to identify which prisoners are likely to re-enter jail between now and end of 2013. We can run our predictive model and identify who is most likely at risk. The is an example of a *binary classification* problem. 

Note the outcome window of 5 years is completely arbitrary. You could use a window of 5, 3, 1 years or 1 day. 

In order to predict recidivism, we will be using data from the `inmate` and `sentences` table to create labels (predictors, or independent variables, or $X$ variables) and features (dependent variables, or $Y$ variables). 

We need to munge our data into **labels** (1_Machine_Learning_Labels.rmd) and **features** (2_Machine_Learning_Features.rmd) before we can train and evaluate **machine learning models** (3_Machine_Learning_Models.rmd).

This tutorial assumes that you have already worked through the `1_Machine_Learning_Labels` and `2_Machine_Learning_Features` tutorials. In the following chunk, we will recreate the two function `create_labels` and `create_features` again.

```{r warning=FALSE}
# create labels function
create_labels <- function(features_end, prediction_start, prediction_end, conn) {
  # @param features_end
  # @param prediction_start
  # @param prediction_end
  # @param conn: obj
  
  end_x_year = format(as.Date(features_end, format="%Y-%m-%d"),"%Y")
  start_y_year = format(as.Date(prediction_start, format="%Y-%m-%d"),"%Y")
  end_y_year = format(as.Date(prediction_end, format="%Y-%m-%d"),"%Y")
  
  drop_script_1 = "drop table if exists sentences_prep;"
  sql_script_1 = glue("create table sentences_prep as
select inmate_doc_number, cast(inmate_sentence_component as integer) 
as sentence_component, date([sentence_begin_date_(for_max)]) 
as sentence_begin_date,
date(actual_sentence_end_date) as sentence_end_date
from sentences;")

  drop_script_2 = glue("drop table if exists release_dates_2000_{end_x_year};")
  sql_script_2 = glue("create temp table release_dates_2000_{end_x_year} as
select inmate_doc_number, sentence_end_date
from sentences_prep where sentence_end_date >= '2000-01-01' and 
sentence_end_date <= '{features_end}';")
  
  drop_script_3 = glue("drop table if exists last_exit_2000_{end_x_year};")
  sql_script_3 = glue("create temp table last_exit_2000_{end_x_year} as 
select inmate_doc_number, max(sentence_end_date) sentence_end_date 
from release_dates_2000_{end_x_year} group by inmate_doc_number;")

  drop_script_4 = glue("drop table if exists admit_{start_y_year}_{end_y_year};")
  sql_script_4 = glue("create temp table admit_{start_y_year}_{end_y_year} as 
select inmate_doc_number, sentence_component, sentence_begin_date 
from sentences_prep where sentence_begin_date >= '{prediction_start}' 
and sentence_begin_date <= '{prediction_end}' and sentence_component = 1;")

  drop_script_5 = glue("drop table if exists recidivism_{start_y_year}_{end_y_year};")
  sql_script_5 = glue("create temp table recidivism_{start_y_year}_{end_y_year} as
select r.inmate_doc_number, r.sentence_end_date, a.sentence_begin_date, case 
when a.sentence_begin_date is null then 0 else 1 end recidivism 
from last_exit_2000_{end_x_year} r
left join admit_{start_y_year}_{end_y_year} a on r.inmate_doc_number = a.inmate_doc_number;")
  
  drop_script_6 = glue("drop table if exists recidivism_labels_{start_y_year}_{end_y_year};")
  sql_script_6 = glue("
create table recidivism_labels_{start_y_year}_{end_y_year} as
select distinct inmate_doc_number, recidivism
from recidivism_{start_y_year}_{end_y_year};")
  
  DBI::dbSendStatement(conn, drop_script_1)
  DBI::dbSendQuery(conn, sql_script_1)
  
  DBI::dbSendStatement(conn, drop_script_2)
  DBI::dbSendQuery(conn, sql_script_2)
  
  DBI::dbSendStatement(conn, drop_script_3)
  DBI::dbSendQuery(conn, sql_script_3)
  
  DBI::dbSendStatement(conn, drop_script_4)
  DBI::dbSendQuery(conn, sql_script_4)
  
  DBI::dbSendStatement(conn, drop_script_5)
  DBI::dbSendQuery(conn, sql_script_5)
  
  DBI::dbSendStatement(conn, drop_script_6)
  DBI::dbSendQuery(conn, sql_script_6)

  sql_query = glue("select * from recidivism_labels_{start_y_year}_{end_y_year}")
  df_label = data.frame(tbl(conn, sql(sql_query)))
  
  return(df_label)
}
```

```{r warning=FALSE}
# create features function
create_features <- function(features_end, prediction_start, prediction_end, conn) {
  # @param features_end
  # @param prediction_start
  # @param prediction_end
  # @param conn: obj
  end_x_year = format(as.Date(features_end, format="%Y-%m-%d"),"%Y")
  start_y_year = format(as.Date(prediction_start, format="%Y-%m-%d"),"%Y")
  end_y_year = format(as.Date(prediction_end, format="%Y-%m-%d"),"%Y")

  drop_script_1 = "drop table if exists sentences_prep;"
  sql_script_1 = glue("create table sentences_prep as
select inmate_doc_number, 
cast(inmate_sentence_component as integer) as sentence_component,
date([sentence_begin_date_(for_max)]) as sentence_begin_date,
date(actual_sentence_end_date) as sentence_end_date
from sentences;")
  
  drop_script_2 = glue("drop table if exists feature_num_admits_2000_{end_x_year};")
  sql_script_2 = glue("create table feature_num_admits_2000_{end_x_year} as
select inmate_doc_number, count(*) num_admits from sentences_prep
where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{start_y_year}_{end_y_year})
and sentence_begin_date < '{features_end}' and sentence_component = 1
group by inmate_doc_number;")
  
  drop_script_3 = glue("drop table if exists feature_length_sentence_2000_{end_x_year};")
  sql_script_3 = glue("create table feature_length_sentence_2000_{end_x_year} as
select inmate_doc_number, sentence_component, cast(julianday(sentence_end_date) - julianday(sentence_begin_date) as integer) length_sentence
from sentences_prep
where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{start_y_year}_{end_y_year})
and sentence_begin_date < '{features_end}' and sentence_component = 1
and sentence_begin_date > '0001-01-01' and sentence_end_date > '0001-01-01' and sentence_end_date > sentence_begin_date;")
  
  drop_script_4 = glue("drop table if exists feature_length_long_sentence_2000_{end_x_year};")
  sql_script_4 = glue("create temp table feature_length_long_sentence_2000_{end_x_year} as
select inmate_doc_number, max(length_sentence) length_longest_sentence
from feature_length_sentence_2000_{end_x_year}
group by inmate_doc_number;")
  
  drop_script_5 = "drop table if exists docnbr_admityr;"
  sql_script_5 = "create temp table docnbr_admityr as
select inmate_doc_number, min(sentence_begin_date) min_admityr
from sentences_prep
where sentence_begin_date > '0001-01-01'
group by inmate_doc_number;"
  
  drop_script_6 = "drop table if exists age_first_admit_birth_year;"
  sql_script_6 = 'create temp table age_first_admit_birth_year as
select da.inmate_doc_number,
cast(strftime("%Y", da.min_admityr) as integer) min_admityr,
cast(strftime("%Y", p.inmate_birth_date) as integer) inmate_birth_date
from docnbr_admityr da
left join inmate p on da.inmate_doc_number = p.inmate_doc_number;'
  
  drop_script_7 = "drop table if exists feature_age_first_admit;"
  sql_script_7 = "create table feature_age_first_admit as
select inmate_doc_number, (min_admityr - inmate_birth_date) age_first_admit
from age_first_admit_birth_year;"
  
  drop_script_8 = "drop table if exists feature_agefirstadmit;"
  sql_script_8 = glue("create table feature_agefirstadmit as
select inmate_doc_number, age_first_admit
from feature_age_first_admit
where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{start_y_year}_{end_y_year});")
  
  drop_script_9 = glue("drop table if exists feature_age_{end_x_year};")
  sql_script_9 = glue('create table feature_age_{end_x_year} as
select inmate_doc_number, ({end_x_year} - cast(strftime("%Y", inmate_birth_date) as integer)) age
from inmate
where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{start_y_year}_{end_y_year});')
  
  drop_script_10 = glue("drop table if exists features_2000_{end_x_year};")
  sql_script_10 = glue('create table features_2000_{end_x_year} as
select f1.inmate_doc_number, f1.num_admits, f2.length_longest_sentence, f3.age_first_admit, f4.age
from feature_num_admits_2000_{end_x_year} f1
left join feature_length_long_sentence_2000_{end_x_year} f2 on f1.inmate_doc_number = f2.inmate_doc_number
left join feature_agefirstadmit f3 on f1.inmate_doc_number = f3.inmate_doc_number
left join feature_age_{end_x_year} f4 on f1.inmate_doc_number = f4.inmate_doc_number;')
  
  DBI::dbSendStatement(conn, drop_script_1)
  DBI::dbSendStatement(conn, sql_script_1)
  
  DBI::dbSendStatement(conn, drop_script_2)
  DBI::dbSendStatement(conn, sql_script_2)
  
  DBI::dbSendStatement(conn, drop_script_3)
  DBI::dbSendStatement(conn, sql_script_3)
  
  DBI::dbSendStatement(conn, drop_script_4)
  DBI::dbSendStatement(conn, sql_script_4)
  
  DBI::dbSendStatement(conn, drop_script_5)
  DBI::dbSendStatement(conn, sql_script_5)
  
  DBI::dbSendStatement(conn, drop_script_6)
  DBI::dbSendStatement(conn, sql_script_6)
  
  DBI::dbSendStatement(conn, drop_script_7)
  DBI::dbSendStatement(conn, sql_script_7)
  
  DBI::dbSendStatement(conn, drop_script_8)
  DBI::dbSendStatement(conn, sql_script_8)
  
  DBI::dbSendStatement(conn, drop_script_9)
  DBI::dbSendStatement(conn, sql_script_9)
  
  DBI::dbSendStatement(conn, drop_script_10)
  DBI::dbSendStatement(conn, sql_script_10)
  
  sql_query = glue("select * from features_2000_{end_x_year}")
  df_features = data.frame(tbl(conn, sql(sql_query)))
  
  return(df_features)}
```
```{r, include=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
# These functions make sure that the tables have been created in the database.
create_labels('2008-12-31', '2009-01-01', '2013-12-31', conn)
create_labels('2013-12-31', '2014-01-01', '2018-12-31', conn)

create_features('2008-12-31', '2009-01-01', '2013-12-31', conn)
create_features('2013-12-31', '2014-01-01', '2018-12-31', conn)
```

# Create Training and Test Sets
---

The Machine Learning framework we will be using is called "mlr3", which provides an Efficient, object-oriented programming platform on the building blocks of machine learning. In order to start training and evaluation with "mlr3", we are required to follow these procedures listed:
- Define the __Task__.
- Define the __Learner__.
- Implement the training.
- Implement the prediction.
- Evaluate the trained model with multiple __Measures__.

However, the "mlr3" machine learning framework utilize a resampling framework with cross-validation, bootstrap, etc, included, which does not necessarily take in the training and test splitting beforehand. As an outcome, we will modify the initial configurations according to the python notebooks and combine the training and test dataset as a whole so as to facilitate the `mlr3` training framework.

### Our Training Set

We create a training set that takes people at the beginning of 2009 and defines the outcome based on data from 2009-2013 (`recidivism_labels_2009_2013`). The features for each person are based on data up to the end of 2008 (`features_2000_2008`).

*Note:* It is important to segregate your data based on time when creating features. Otherwise there can be "leakage", where you accidentally use information that you would not have known at the time.

```{r}
sql_string = "drop table if exists train_matrix;"
DBI::dbSendStatement(conn, sql_string)
```
```{r}
sql_string = "
create table train_matrix as 
select l.inmate_doc_number, l.recidivism, f.num_admits, f.length_longest_sentence, f.age_first_admit, f.age 
from recidivism_labels_2009_2013 l 
left join features_2000_2008 f on f.inmate_doc_number = l.inmate_doc_number;"
DBI::dbSendQuery(conn, sql_string)
```

We then load the training data into `df_training`.

```{r}
sql_string = "SELECT * FROM train_matrix"

df_training = data.frame(tbl(conn, sql(sql_string)))
head(df_training, n=5)
```

### Our Test (Validation) Set

In the machine learning process, we want to build models on the training set and evaluate them on the test set. Our test set will use labels from 2014-2018 (`recidivism_labels_2014_2018`), and our features will be based on data up to the end of 2013 (`features_2000_2013`). 

```{r}
sql_string = "drop table if exists test_matrix;"
DBI::dbSendStatement(conn, sql_string)
```
```{r}
sql_string = "
create table test_matrix as 
select l.inmate_doc_number, l.recidivism, f.num_admits, f.length_longest_sentence, f.age_first_admit, f.age 
from recidivism_labels_2014_2018 l 
left join features_2000_2013 f on f.inmate_doc_number = l.inmate_doc_number;"
DBI::dbSendQuery(conn, sql_string)
```

We load the test data into `df_test`.

```{r}
sql_string = "SELECT * FROM test_matrix"

df_test = data.frame(tbl(conn, sql(sql_string)))
head(df_test, n=5)
```

The next step would be to join the training and test dataset vertically.
```{r}
total_df <- rbind(df_training, df_test)
nrow(total_df)
```

### Data Cleaning

Before we proceed to model training, we need to clean our data. First, we check the percentage of missing values.

```{r}
# count the number of missing values row wise
isnan_totaldf_rows = sum(!complete.cases(total_df))

nrows_totaldf = nrow(total_df)

# count the missing value percentage
missing_percent = isnan_totaldf_rows/nrows_totaldf
missing = glue("missing rows ratio:{missing_percent}")
cat(missing)
```
We see that about 1% of the rows in our data have missing values. In the following, we will drop rows with missing values. Note, however, that better ways for dealing with missing values exist, e.g., general data imputation methods.

```{r}
total_df = na.omit(total_df)
cleaned_df = nrow(total_df) # number of rows after cleaning
```
```{r}
c(nrows_totaldf, cleaned_df) # checking whether the missing data are dropped
```
Let's check if the values of the ages at first admit are reasonable.

```{r}
length(unique(total_df$age_first_admit))
length(total_df$age_first_admit)
```
Looks like this needs some cleaning. We will drop any rows that have age < 14 and > 99.

```{r}
total_df <- total_df[ which(total_df$age_first_admit >= 14
& total_df$age_first_admit <= 99), ]
```
Let's check how much data we still have and how many examples of recidivism are in our training dataset. When it comes to model evaluation, it is good to know what the "baseline" is in our dataset.

```{r}
row_after_cleaning = glue("Number of rows in the dataset: {nrow(total_df)}")
cat(row_after_cleaning)
```
```{r}
count(total_df, total_df$recidivism, sort=TRUE)
```
```{r}
71578/(301704+71578)
301704+71578
```

We have about 37,0000 examples, and about 20% of those are positive examples (recidivist), which is what we're trying to identify. About 80% of the examples are negative examples (non-recidivist).


### Split into features and labels

Here we select our features and outcome variable in a column vector.

```{r}
sel_cols = c('num_admits', 'length_longest_sentence', 'age_first_admit', 'age', 'recidivism')
```

We can now create an X- and y- object to train and evaluate prediction models with `mlr3`.

```{r}
total_df$recidivism = as.factor(total_df$recidivism)
total_data = total_df[sel_cols]
# X_train = df_training[sel_features]
# y_train = df_training[sel_label]
# X_test = df_test[sel_features]
# y_test = df_test[sel_label]
```
```{r}
dim(total_data)
str(total_data)

```
Before the actual training, we can firstly load the required packages.
```{r warning=FALSE}
library("mlr3")
library("mlr3learners")
library("mlr3tuning")
library("mlr3viz")
library("ggplot2")
library("paradox")
library("e1071")
# library('kknn')
library("ranger")
```

In the next step, we need to define the task as specified in the `mlr3` package.

```{r}
task_df = TaskClassif$new("ml_task", total_data, target = "recidivism")

task_df
```

#### Logistic Regression

Also, in order to use the logistic regression model, we are required to initialize the learner.

```{r}
learner_logreg = lrn("classif.log_reg")
print(learner_logreg)
```
In order to perform an efficient splitting of data, one could do the following:
```{r}
train_set = sample(task_df$row_ids, 0.8 * task_df$nrow)
test_set = setdiff(task_df$row_ids, train_set)
```
Then, we have 80% of the data for training and the remained 20% for evaluation.

We can train the model on the train_set by specifiying the row_ids:
```{r warning=FALSE}
learner_logreg$train(task_df, row_ids = train_set)
```
The fitted model can be accessed via:
```{r}
learner_logreg$model
```
```{r}
summary(learner_logreg$model)
```
#### Random Forest
Besides the logistic regression algorithm, we can also train the data with random forest, which implements a feature importance algorithm automatically by supplying the additional importance argument `importance="permutation"`.

```{r}
learner_rf = lrn("classif.ranger", importance = "permutation")
learner_rf$train(task_df, row_ids = train_set)
```
The feature importance values can be seen with `$importance()`:
```{r}
learner_rf$importance()
```

We can also plot the importance values as follows:
```{r}
importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")
```
### Model Prediction

* Predict Class

We can now take a look at the prediction results based on the pre-trained models.
```{r}
pred_logreg = learner_logreg$predict(task_df, row_ids = test_set)
pred_rf = learner_rf$predict(task_df, row_ids = test_set)
```

```{r}
head(as.data.table(pred_logreg), n=5)  # checking the prediction object
```

```{r}
head(as.data.table(pred_rf), n=5)
```
* Predict Probabilities
Most learners may not only predict a class variable (“response”), but also their degree of “belief” / “uncertainty” in a given response. Typically, we achieve this by setting the `$predict_type="prob"` in the learner setting arguments.
```{r}
learner_logreg$predict_type = "prob"
```
```{r}
learner_logreg$predict(task_df, row_ids = test_set)
```


### Performance Evaluation

In order to measure the performance of a learner on test data, we ususally simulate the scenario of these data by splitting them into training and test dataset. In this case, numerous resampling methods can be utilized to repeat the splitting process.

For "mlr3", we can specify the resampling method with the `rsmp()` function:

```{r}
resampling = rsmp("holdout", ratio=2/3)
print(resampling)
```
"holdout" is a simple train-test splitting method. The next step is to use the `resample()` function to implement the resampling calculation.

```{r}
res = resample(task_df, learner = learner_logreg, resampling = resampling)
res
```
The performance score can evaluated using `$aggregate()`:
```{r}
res$aggregate(msrs(c("classif.ce", "classif.auc")))
```
We can also run cross-validation:
```{r}
resampling = rsmp("cv", folds = 5)
rr = resample(task_df, learner = learner_logreg, resampling = resampling)
rr$aggregate(msrs(c("classif.ce", "classif.auc")))
```
### Performance Comparison and Benchmarks
The `benchmark()` function can integrate multiple tasks and learners together. Here, we only show the instance with multiple learners:
```{r}
learners = lrns(c("classif.log_reg", "classif.ranger"), predict_type = "prob")
bm_design = benchmark_grid(
  tasks = task_df,
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)
bmr = benchmark(bm_design)
```
With this benchmark, we can compare different evaluation methods. Here, we focus on the `misclassfication rate` and `AUC`.

```{r}
measures = msrs(c("classif.ce", "classif.auc"))
performances = bmr$aggregate(measures)
performances[, c("learner_id", "classif.ce", "classif.auc")]
```

We can also utilize the `autoplot()` function implemented in the `mlr3viz` package to compare the learners' performances.

```{r}
autoplot(bmr)
```
Also, we can get the ROC-curve with the following command:
```{r}
library('precrec')
autoplot(bmr$clone(deep = TRUE)$filter(task_ids = "ml_task"), type = "roc")
```

```{r}
DBI::dbDisconnect(conn)
```

