{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Problem Formulation](#problem-formulation)\n",
    "- [Creating Labels (Outcomes)](#labels)\n",
    "- [Feature Generation](#features)\n",
    "- [Create Training and Test Sets](#train-test)\n",
    "- [Model Training](#model-training)\n",
    "- [Model Evaluation](#model-evaluation)\n",
    "- [More Models](#more-models)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we'll discuss how to formulate a policy problem or a social science question in the machine learning framework; how to transform raw data into something that can be fed into a model; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `scikit-learn` package in Python. \n",
    "\n",
    "This tutorial is based on chapter \"Machine Learning\" of [Big Data and Social Science](https://coleridge-initiative.github.io/big-data-and-social-science/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from dateutil.parser import parse\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sqlalchemy import create_engine\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be adjusted\n",
    "with open('/home/ckern/nc-data/database_login.yaml') as f:\n",
    "    db_connection_string = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_connection_string)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "---\n",
    "  \n",
    "Our Machine Learning Problem\n",
    ">Of all prisoners released, we would like to predict who is likely to reenter jail within *5* years of the day we make our prediction. For instance, say it is Jan 1, 2012 and we want to identify which \n",
    ">prisoners are likely to re-enter jail between now and end of 2016. We can run our predictive model and identify who is most likely at risk. The is an example of a *binary classification* problem. \n",
    "\n",
    "Note the outcome window of 5 years is completely arbitrary. You could use a window of 5, 3, 1 years or 1 day. \n",
    "\n",
    "In order to predict recidivism, we will be using data from the `...` and `...` table to create **labels** and **features**. \n",
    "\n",
    "We need to munge our dataset into **features** (predictors, or independent variables, or $X$ variables) and **labels** (dependent variables, or $Y$ variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labels (Outcomes)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a table `release_dates_1989_2006`, which is based on the `inmt4bb1` table. We take all of the records for `inmate_doc_number` and `actual_sentence_end_date` between 1989 and 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists release_dates_1989_2006;\n",
    "sql_string = \"create temp table release_dates_1989_2006 as \"\n",
    "sql_string += \"select inmate_doc_number, actual_sentence_end_date, sentence_begin_date_for_max \"\n",
    "sql_string += \"from inmt4bb1 \"\n",
    "sql_string += \"where actual_sentence_end_date >= '1989-01-01' and actual_sentence_end_date < '2006-01-01' \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a table `last_exit_1989_2006`, which takes the *maximum* (most recent) `actual_sentence_end_date` for every `inmate_doc_number` and writes into `last_exit_1989_2006`. This table will only have one entry per `inmate_doc_number`, so for any given `inmate_doc_number`, or individual, we know their *most recent* release year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists last_exit_1989_2006;\n",
    "sql_string = \"create temp table last_exit_1989_2006 as \"\n",
    "sql_string += \"select inmate_doc_number, max(actual_sentence_end_date) actual_sentence_end_date \"\n",
    "sql_string += \"from release_dates_1989_2006 \"\n",
    "sql_string += \"group by inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find everyone admitted into prison between 2006 and 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists admit_2006_2010;\n",
    "sql_string = \"create temp table admit_2006_2011 as \"\n",
    "sql_string += \"select inmate_doc_number, sentence_begin_date_for_max \"\n",
    "sql_string += \"from inmt4bb1 \"\n",
    "sql_string += \"where sentence_begin_date_for_max >= '2006-01-01' and sentence_begin_date_for_max < '2011-01-01' and inmate_sentence_component = 1 \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do a `left join`  on the `last_exit_1989_2006` (left) table and the `recidivism_2006_2010` (right) table on the `inmate_doc_number` field. The resulting table will keep all the entries from the *left* table (most recent releases between 1989 and 2006) and add their admits between 2006 and 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists recidivism_2005_2010;\n",
    "sql_string = \"create temp table recidivism_2006_2011 as \"\n",
    "sql_string += \"select r.inmate_doc_number, r.actual_sentence_end_date, a.sentence_begin_date_for_max, \"\n",
    "sql_string += \"case when a.sentence_begin_date_for_max is null then 0 else 1 end recidivism \"\n",
    "sql_string += \"from last_exit_1989_2006 r \"\n",
    "sql_string += \"left join admit_2006_2011 a on r.inmate_doc_number = a.inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a label: 0 indicates *no recidivism*, 1 indicates that person did return to jail within the outcome period (beginning of 2006 to end 2010). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists recidivism_labels_2005_2010;\n",
    "sql_string = \"create table public.recidivism_labels_2006_2011 as \"\n",
    "sql_string += \"select distinct inmate_doc_number, recidivism \"\n",
    "sql_string += \"from recidivism_2006_2011 \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT *\"\n",
    "sql_string += \"FROM recidivism_labels_2006_2011 \"\n",
    "sql_string += \";\"\n",
    "\n",
    "label_2006_2011 = pd.read_sql(sql_string, con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2006_2011.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(prediction_start, prediction_end, conn):\n",
    "    \"\"\"\n",
    "    Generate a list of labels and return the table as a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_start\n",
    "    prediction_end\n",
    "    conn: obj\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_labels: DataFrame\n",
    "    \"\"\"\n",
    "    begin_range = prediction_start\n",
    "    end_range = prediction_end\n",
    "    begin_year = parse(begin_range, fuzzy=True).year\n",
    "    end_year = parse(end_range, fuzzy=True).year\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql_script=\"\"\"\n",
    "\n",
    "drop table if exists release_dates_1989_{begin_year};\n",
    "create temp table release_dates_1989_{begin_year} as\n",
    "select inmate_doc_number, actual_sentence_end_date, sentence_begin_date_for_max\n",
    "from inmt4bb1\n",
    "where actual_sentence_end_date >= '1989-01-01' and actual_sentence_end_date < '{begin_range}';\n",
    "commit;\n",
    "\n",
    "drop table if exists last_exit_1989_{begin_year};\n",
    "create temp table last_exit_1989_{begin_year} as\n",
    "select inmate_doc_number, max(actual_sentence_end_date) actual_sentence_end_date\n",
    "from release_dates_1989_{begin_year}\n",
    "group by inmate_doc_number;\n",
    "commit;\n",
    "\n",
    "drop table if exists admit_{begin_year}_{end_year};\n",
    "create temp table admit_{begin_year}_{end_year} as\n",
    "select inmate_doc_number, sentence_begin_date_for_max\n",
    "from inmt4bb1\n",
    "where sentence_begin_date_for_max >= '{begin_range}' and sentence_begin_date_for_max < '{end_range}' and inmate_sentence_component = 1;\n",
    "commit;\n",
    "\n",
    "drop table if exists recidivism_{begin_year}_{end_year};\n",
    "create temp table recidivism_{begin_year}_{end_year} as\n",
    "select r.inmate_doc_number, r.actual_sentence_end_date, a.sentence_begin_date_for_max,\n",
    "case when a.sentence_begin_date_for_max is null then 0 else 1 end recidivism\n",
    "from last_exit_1989_{begin_year} r\n",
    "left join admit_{begin_year}_{end_year} a on r.inmate_doc_number = a.inmate_doc_number;\n",
    "commit;\n",
    "\n",
    "drop table if exists recidivism_labels_{begin_year}_{end_year};\n",
    "create table recidivism_labels_{begin_year}_{end_year} as\n",
    "select distinct inmate_doc_number, recidivism\n",
    "from recidivism_{begin_year}_{end_year};\n",
    "commit; \n",
    "\n",
    "    \"\"\".format(begin_range=begin_range,\n",
    "               end_range=end_range,\n",
    "               begin_year=begin_year,\n",
    "               end_year=end_year)\n",
    "    \n",
    "    cursor.execute(sql_script)\n",
    "    df_label = pd.read_sql('select * from recidivism_labels_{begin_year}_{end_year}'.format(\n",
    "                                                                                    begin_year=begin_year,\n",
    "                                                                                    end_year=end_year), conn)    \n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2011_2016 = create_labels('2011-01-01', '2016-01-01', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2011_2016.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "---\n",
    "\n",
    "Our preliminary features are the following\n",
    "\n",
    "- `nadmits`: Number of times someone has been addmitted to prison between 1989-2005. The more times someone has been to prison the more times they are likely continue to be arrested. \n",
    "\n",
    "- `length_sentence`: The length of the longest sentence of all admits between 1989-2005.\n",
    "\n",
    "- `age_first_admit`: The age someone was first admitted to prison. This is calculated by subtracting their `birth_yr` from the year they were first admitted into prison. The idea behind creating this feature is that people who are younger when they are first arrested are more likely to be arrested again. \n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of admits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists feature_nadmits_1989_2005;\n",
    "sql_string = \"create table feature_num_admits_1989_2006 as \"\n",
    "sql_string += \"select inmate_doc_number, count(*) num_admits \"\n",
    "sql_string += \"from inmt4bb1 \"\n",
    "sql_string += \"where inmate_doc_number in (select inmate_doc_number from recidivism_labels_2006_2011) \"\n",
    "sql_string += \"and sentence_begin_date_for_max >= '1988-01-01' and sentence_begin_date_for_max < '2006-01-01' and inmate_sentence_component = 1 \" \n",
    "sql_string += \"group by inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists feature_length_sentence_1989_2006;\n",
    "sql_string = \"create table feature_length_sentence_1989_2006 as \"\n",
    "sql_string += \"select inmate_doc_number, inmate_sentence_component, (actual_sentence_end_date - sentence_begin_date_for_max) length_sentence \"\n",
    "sql_string += \"from inmt4bb1 \"\n",
    "sql_string += \"where inmate_doc_number in (select inmate_doc_number from recidivism_labels_2006_2011) \"\n",
    "sql_string += \"and sentence_begin_date_for_max >= '1988-01-01' and sentence_begin_date_for_max < '2006-01-01' and inmate_sentence_component = 1 \" \n",
    "sql_string += \"and sentence_begin_date_for_max > '0001-01-01' and actual_sentence_end_date > '0001-01-01' and actual_sentence_end_date > sentence_begin_date_for_max \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists feature_length_long_sentence_1989_2006;\n",
    "sql_string = \"create temp table feature_length_long_sentence_1989_2006 as \"\n",
    "sql_string += \"select inmate_doc_number, max(length_sentence) length_longest_sentence \"\n",
    "sql_string += \"from feature_length_sentence_1989_2006 \"\n",
    "sql_string += \"group by inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age at first arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists docnbr_admityr;\n",
    "sql_string = \"create temp table docnbr_admityr as \"\n",
    "sql_string += \"select inmate_doc_number, min(sentence_begin_date_for_max) min_admityr \"\n",
    "sql_string += \"from inmt4bb1 \"\n",
    "sql_string += \"where sentence_begin_date_for_max > '0001-01-01' \"\n",
    "sql_string += \"group by inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists age_first_admit_birth_year;\n",
    "sql_string = \"create temp table age_first_admit_birth_year as \"\n",
    "sql_string += \"select da.inmate_doc_number, extract(year from da.min_admityr) min_admityr, extract(year from p.offender_birth_date) offender_birth_date \"\n",
    "sql_string += \"from docnbr_admityr da \"\n",
    "sql_string += \"left join ofnt3aa1 p on da.inmate_doc_number = p.offender_nc_doc_id_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop table if exists feature_age_first_admit;\n",
    "sql_string = \"create table feature_age_first_admit as \"\n",
    "sql_string += \"select inmate_doc_number, (min_admityr - offender_birth_date) age_first_admit \"\n",
    "sql_string += \"from age_first_admit_birth_year \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table if exists feature_agefirstadmit;\n",
    "sql_string = \"create table feature_agefirstadmit as \"\n",
    "sql_string += \"select inmate_doc_number, age_first_admit \"\n",
    "sql_string += \"from feature_age_first_admit \"\n",
    "sql_string += \"where inmate_doc_number in (select inmate_doc_number from feature_num_admits_1989_2006) \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table if exists features_1989_2006;\n",
    "sql_string = \"create table features_1989_2006 as \"\n",
    "sql_string += \"select f1.inmate_doc_number, f1.num_admits, f2.length_longest_sentence, f3.age_first_admit \"\n",
    "sql_string += \"from feature_num_admits_1989_2006 f1 \"\n",
    "sql_string += \"left join feature_length_long_sentence_1989_2006 f2 on f1.inmate_doc_number = f2.inmate_doc_number \"\n",
    "sql_string += \"left join feature_agefirstadmit f3 on f1.inmate_doc_number = f3.inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT *\"\n",
    "sql_string += \"FROM features_1989_2006 \"\n",
    "sql_string += \";\"\n",
    "\n",
    "features_1989_2006 = pd.read_sql(sql_string, con = conn)\n",
    "features_1989_2006.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(prediction_start, prediction_end, conn):\n",
    "    \"\"\"\n",
    "    Generate a list of features and return the table as a dataframe.\n",
    "    Note: There has to be a table of labels that correspond with the same time period. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_date\n",
    "    prediction_end\n",
    "    conn: obj\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_features: Dataframe\n",
    "    \"\"\"\n",
    "    begin_range = prediction_start\n",
    "    end_range = prediction_end\n",
    "    begin_year = parse(begin_range, fuzzy=True).year\n",
    "    end_year = parse(end_range, fuzzy=True).year \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    sql_script=\"\"\"\n",
    "\n",
    "drop table if exists feature_num_admits_1989_{begin_year};\n",
    "create table feature_num_admits_1989_{begin_year} as \n",
    "select inmate_doc_number, count(*) num_admits\n",
    "from inmt4bb1\n",
    "where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{begin_year}_{end_year})\n",
    "and sentence_begin_date_for_max >= '1988-01-01' and sentence_begin_date_for_max < '{begin_range}' and inmate_sentence_component = 1\n",
    "group by inmate_doc_number; \n",
    "commit; \n",
    "\n",
    "drop table if exists feature_length_sentence_1989_{begin_year};\n",
    "create table feature_length_sentence_1989_{begin_year} as\n",
    "select inmate_doc_number, inmate_sentence_component, (actual_sentence_end_date - sentence_begin_date_for_max) length_sentence\n",
    "from inmt4bb1\n",
    "where inmate_doc_number in (select inmate_doc_number from recidivism_labels_{begin_year}_{end_year})\n",
    "and sentence_begin_date_for_max >= '1988-01-01' and sentence_begin_date_for_max < '2006-01-01' and inmate_sentence_component = 1\n",
    "and sentence_begin_date_for_max > '0001-01-01' and actual_sentence_end_date > '0001-01-01' and actual_sentence_end_date > sentence_begin_date_for_max ;\n",
    "commit; \n",
    "\n",
    "drop table if exists feature_length_long_sentence_1989_{begin_year};\n",
    "create temp table feature_length_long_sentence_1989_{begin_year} as\n",
    "select inmate_doc_number, max(length_sentence) length_longest_sentence\n",
    "from feature_length_sentence_1989_{begin_year}\n",
    "group by inmate_doc_number;\n",
    "commit; \n",
    "\n",
    "drop table if exists docnbr_admityr;\n",
    "create temp table docnbr_admityr as\n",
    "select inmate_doc_number, min(sentence_begin_date_for_max) min_admityr\n",
    "from inmt4bb1\n",
    "where sentence_begin_date_for_max > '0001-01-01'\n",
    "group by inmate_doc_number;\n",
    "commit; \n",
    "\n",
    "drop table if exists age_first_admit_birth_year;\n",
    "create temp table age_first_admit_birth_year as\n",
    "select da.inmate_doc_number, extract(year from da.min_admityr) min_admityr, extract(year from p.offender_birth_date) offender_birth_date\n",
    "from docnbr_admityr da\n",
    "left join ofnt3aa1 p on da.inmate_doc_number = p.offender_nc_doc_id_number;\n",
    "commit; \n",
    "\n",
    "drop table if exists feature_age_first_admit; \n",
    "create table feature_age_first_admit as \n",
    "select inmate_doc_number, (min_admityr - offender_birth_date) age_first_admit\n",
    "from age_first_admit_birth_year;\n",
    "commit; \n",
    "\n",
    "drop table if exists feature_agefirstadmit; \n",
    "create table feature_agefirstadmit as\n",
    "select inmate_doc_number, age_first_admit\n",
    "from feature_age_first_admit\n",
    "where inmate_doc_number in (select inmate_doc_number from feature_num_admits_1989_{begin_year});\n",
    "commit; \n",
    "\n",
    "drop table if exists features_1989_{begin_year}; \n",
    "create table features_1989_{begin_year} as\n",
    "select f1.inmate_doc_number, f1.num_admits, f2.length_longest_sentence, f3.age_first_admit\n",
    "from feature_num_admits_1989_{begin_year} f1\n",
    "left join feature_length_long_sentence_1989_{begin_year} f2 on f1.inmate_doc_number = f2.inmate_doc_number\n",
    "left join feature_agefirstadmit f3 on f1.inmate_doc_number = f3.inmate_doc_number;\n",
    "commit; \n",
    "\n",
    "    \"\"\".format(begin_range=begin_range, \n",
    "               end_range = end_range,\n",
    "               begin_year = begin_year,\n",
    "               end_year = end_year)\n",
    "    \n",
    "    cursor.execute(sql_script)\n",
    "    df_features = pd.read_sql('select * from features_1989_{begin_year}'.format(begin_year=begin_year), conn)    \n",
    "    return df_features     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1989_2011 = create_features('2011-01-01', '2016-01-01', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1989_2011.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Test Sets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Training Set\n",
    "\n",
    "We are going to create a training set that will take people at the beginning of 2006 and will generate labels for them based on data from 2006-2010. The features for each person are created based on data from the beginnig of our  data (1989) up to the end of 2005.\n",
    "\n",
    "*Note:* it is important to segregate your data based on time when creating features. Otherwise there can be \"leakage,\" where you accidentally use information that you would not have known at the time.  This happens often when calculating aggregation features; for instance, it is quite easy to calculate an average using values that go beyond our training set time-span and not realize it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"create table train_matrix as \"\n",
    "sql_string += \"select l.inmate_doc_number, l.recidivism, f.num_admits, f.length_longest_sentence, f.age_first_admit \"\n",
    "sql_string += \"from recidivism_labels_2006_2011 l \"\n",
    "sql_string += \"left join features_1989_2006 f on f.inmate_doc_number = l.inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT *\"\n",
    "sql_string += \"FROM train_matrix \"\n",
    "sql_string += \";\"\n",
    "\n",
    "df_training = pd.read_sql(sql_string, con = conn)\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Test (Validation) Set\n",
    "\n",
    "We will then take the model built on that training set and validate it on the Test Set. Our testing set will use labels from 2011-2015, and our features will be generated from 1989-2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"create table test_matrix as \"\n",
    "sql_string += \"select l.inmate_doc_number, l.recidivism, f.num_admits, f.length_longest_sentence, f.age_first_admit \"\n",
    "sql_string += \"from recidivism_labels_2011_2016 l \"\n",
    "sql_string += \"left join features_1989_2011 f on f.inmate_doc_number = l.inmate_doc_number \"\n",
    "sql_string += \";\"\n",
    "\n",
    "cur.execute(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = \"SELECT *\"\n",
    "sql_string += \"FROM test_matrix \"\n",
    "sql_string += \";\"\n",
    "\n",
    "df_testing = pd.read_sql(sql_string, con = conn)\n",
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Before we proceed to model training, we check the percentage of missing values in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_training_rows = df_training.isnull().any(axis=1)\n",
    "nrows_training = df_training.shape[0]\n",
    "nrows_training_isnan = df_training[isnan_training_rows].shape[0]\n",
    "print('%of frows with NaNs {} '.format(float(nrows_training_isnan)/nrows_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that about 4% of the rows in our training set have missing values. In our example, we will drop rows with missing values. In practice, better ways for dealing with missings exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df_training[~isnan_training_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of the ages at first admit are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique( df_training['age_first_admit'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop any rows that have age <= 15 and >= 99.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = (df_training['age_first_admit'] > 15) & (df_training['age_first_admit'] < 99)\n",
    "df_training = df_training[keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how much data we still have and how many examples of recidivism are in our training dataset. We don't necessarily need to have a perfect balance of recidivists and non-recivists, but it's good to know what the \"baseline\" is in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_training.shape[0]))\n",
    "df_training['recidivism'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 200,000 examples, and about 20% of those are *positive* examples (recidivist), which is what we're trying to identify. About 80% of the examples are *negative* examples (non-recidivst). Let's take a look at our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnan_testing_rows = df_testing.isnull().any(axis=1)\n",
    "nrows_testing = df_testing.shape[0]\n",
    "nrows_testing_isnan = df_testing[isnan_testing_rows].shape[0]\n",
    "print('%of rows with NaNs {} '.format(float(nrows_testing_isnan)/nrows_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that about 3% of the rows in our testing set have missing values. This matches what we'd expect based on what we saw in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = df_testing[~isnan_testing_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As before, we drop cases with age <= 15 and >= 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = (df_testing['age_first_admit'] > 15) & (df_testing['age_first_admit'] < 99)\n",
    "df_testing = df_testing[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_testing.shape[0]))\n",
    "df_testing['recidivism'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_features = ['num_admits', 'length_longest_sentence', 'age_first_admit']\n",
    "sel_label = 'recidivism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_training[sel_features].values\n",
    "y_train = df_training[sel_label].values\n",
    "X_test = df_testing[sel_features].values\n",
    "y_test = df_testing[sel_label].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty = 'l1', C = 1e5)\n",
    "model.fit( X_train, y_train )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model results, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `penalty`, `C` , and `intercept_scaling`).\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `penalty` of \"elasticnet\", `C` of 0.01, and `intercept_scaling` of 2 (as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression(penalty = 'elasticnet', C = 0.01, intercept_scaling = 2)\n",
    "\n",
    "The basic way to choose values for, or \"tune,\" these parameters is the same as the way you choose a model: fit the model to your training data with a variety of parameters, and see which perform the best on the test set. An obvious drawback is that you can also *overfit* to your test set; in this case, you can alter your method of cross-validation.\n",
    "\n",
    "Let's look at what the model learned and what the coefficients are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "std_coef = np.std(X_test,0)*model.coef_\n",
    "std_coef[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "---\n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score (that can sometimes be interpreted a a probabilty) between 0 and 1, which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of scores and see if it makes sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our distribution of scores is skewed, with the majority of scores on the lower end of the scale. We expect this because 75% of the data is made up of nonrecidivists, so we'd guess that a higher proportion of the examples in the test set will be negative (meaning they should have lower scores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. As a trial run, let's set a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,0.5) for score in y_scores] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy score of 84%. Recall that our testing dataset had 85% non-recidivists and 15% recidivists. If we had just labeled all the examples as negative and guessed non-recidivist every time, we would have had an accuracy of 85%, so our basic model is not doing better than a \"dumb classifier\". That's ok, because we're just getting started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-PR and AUC-ROC\n",
    "\n",
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(expected, y_scores)\n",
    "auc_val = auc(recall_curve,precision_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall_curve, precision_curve)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(expected, y_scores)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k%\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a special program for people likely to be recidivists, but only 5% can be admitted. In that case, we would want to prioritize the 5% who were *most likely* to end up back in jail, and it wouldn't matter too much how accurate we were on the 80% or so who weren't very likely to end up back in jail. \n",
    "\n",
    "Let's say that, out of the approximately 200,000 prisoners, we can intervene on 5% of them, or the \"top\" 10,000 prisoners (where \"top\" means highest predicted risk of recidivism). We can then focus on optimizing our **precision at 5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls\n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_5 = precision_at_k(expected,y_scores, 0.05)\n",
    "print('Precision at 5%: {:.2f}'.format(p_at_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline \n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 83% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone a non-recividist, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.05)\n",
    "random_p_at_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More models\n",
    "---\n",
    "\n",
    "We have only scratched the surface of what we can do with our model. We've only tried one classifier (Logistic Regression), and there are plenty more classification algorithms in `sklearn`. Let's try them! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train )\n",
    "    print(clf)\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_5 = precision_at_k(expected,y_score, 0.05)\n",
    "    if max_p_at_k < p_at_5:\n",
    "        max_p_at_k = p_at_5\n",
    "    print('Precision at 5%: {:.2f}'.format(p_at_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some of the models we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random forest RF\n",
    "sel_clfs\n",
    "clf = clfs[sel_clfs[0]]\n",
    "#clf = clfs[clfNM]\n",
    "print(clf)\n",
    "clf.fit( X_train, y_train )\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can make this look a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std ([tree.feature_importances_ for tree in clf.estimators_],\n",
    "       axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print (\"Feature ranking\")\n",
    "for f in range(X_test.shape[1]):\n",
    "    print (\"%d. %s (%f)\" % (f + 1, sel_features[f], importances[indices[f]]))\n",
    "\n",
    "# plot \n",
    "plt.figure\n",
    "plt.title (\"Feature Importances\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices], color='r',\n",
    "      yerr=std[indices], align = \"center\")\n",
    "plt.xticks(range(X_test.shape[1]), sel_features, rotation=90)\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has just scratched the surface. Try the following: \n",
    "    \n",
    "- Create more features\n",
    "- Try more models\n",
    "- Try different parameters for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
